{
 "metadata": {
  "name": "",
  "signature": "sha256:7399fe751422836c6eb158bea4e2576190314484a0fa3fef9d19e4f1540328ca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print(sys.version)\n",
      "import pandas as pd\n",
      "import math\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import datetime\n",
      "import bson\n",
      "import gzip\n",
      "import pickle\n",
      "from collections import Counter\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.7.7 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 10:40:02) [MSC v.1500 64 bit (AMD64)]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path_list = ['all_texts/' + str(x) for x in pd.read_csv('first_level_sample_list_exists')['0']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using rosetta"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "https://github.com/columbia-applied-data-science/rosetta/blob/master/examples/vw_helpers.md"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rosetta.text.text_processors import BaseTokenizer\n",
      "import rosetta.text.nlp\n",
      "class TokenizerBasic3(BaseTokenizer):\n",
      "    def text_to_token_list(self, text):\n",
      "        tokens = rosetta.text.nlp.word_tokenize(text, L=3, numeric=False)\n",
      "        return [word.lower() for word in tokens if not rosetta.text.nlp.is_stopword(word)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#my_tokenizer.text_to_token_list('werwertrew ty erth tyh rtunjfgn jfyuj rujyfun')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#?TokenizerBasic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rosetta import TextFileStreamer, TokenizerBasic, MakeTokenizer\n",
      "\n",
      "#my_tokenizer = TokenizerBasic()\n",
      "my_tokenizer = TokenizerBasic3()\n",
      "#my_tokenizer = MakeTokenizer(clean)\n",
      "#stream = TextFileStreamer(tokenizer=my_tokenizer, path_list=path_list)#text_base_path \n",
      "stream = TextFileStreamer(tokenizer=my_tokenizer, text_base_path='all_texts') \n",
      "stream.to_vw('doc_tokens_all.vw', n_jobs=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from rosetta import TextFileStreamer, TokenizerBasic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sff.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rosetta.text.text_processors import SFileFilter, VWFormatter\n",
      "sff = SFileFilter(VWFormatter())\n",
      "sff.load_sfile('doc_tokens_all.vw')\n",
      "\n",
      "df = sff.to_frame()\n",
      "df.head()\n",
      "df.describe()\n",
      "\n",
      "sff.filter_extremes(doc_freq_min=5, doc_fraction_max=0.8)\n",
      "sff.compactify()\n",
      "sff.save('sff_file_all.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Removed 820030/977013 tokens\n",
        "Compactification done.  self.bit_precision_required = 18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "collisions = 0, vocab_size = 156983"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "All collisions resolved"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sff.filter_sfile('doc_tokens_all.vw', 'doc_tokens_filtered_all.vw')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counter = 0\n",
      "with open('doc_tokens_filtered_all.vw', 'r') as f_in:\n",
      "    with open('doc_tokens_filtered_all_fixed.vw', 'w') as f_out:\n",
      "        for line in f_in:\n",
      "            if line[-3] == '|':\n",
      "                counter += 1\n",
      "            else:\n",
      "                words_count = 0\n",
      "                for x in line.split('|')[1].split(' '):\n",
      "                    words_count += int(x.split(':')[1])\n",
      "                if words_count > \n",
      "                f_out.write(line)\n",
      "                #print(line, file=f_out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "1218"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw.exe --lda 20 --cache_file ddrs.cache --passes 10 -p prediction_all.dat --readable_model topics_all.dat --bit_precision 18 doc_tokens_filtered_all_fixed.vw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Num weight bits = 18\n",
        "learning rate = 0.5\n",
        "initial_t = 0\n",
        "power_t = 0.5\n",
        "decay_learning_rate = 1\n",
        "predictions = prediction_all.dat\n",
        "can't open: ddrs.cache, error = No such file or directory\n",
        "creating cache_file = ddrs.cache\n",
        "Reading datafile = doc_tokens_filtered_all_fixed.vw\n",
        "num sources = 1\n",
        "average    since         example     example  current  current  current\n",
        "loss       last          counter      weight    label  predict features\n",
        "13.194272  13.194272         1      1.0     1.0000   0.0000      105\n",
        "13.157826  13.121380         2      2.0     1.0000   0.0000       39\n",
        "13.175115  13.192404         4      4.0     1.0000   0.0000       69\n",
        "12.189789  11.204464         8      8.0     1.0000   0.0000      217\n",
        "11.272938  10.356087        16     16.0     1.0000   0.0000      123\n",
        "10.895573  10.518208        32     32.0     1.0000   0.0000       46\n",
        "10.806581  10.717589        64     64.0     1.0000   0.0000        1\n",
        "10.331567  9.856554        128    128.0     1.0000   0.0000      107\n",
        "10.249196  10.166825       256    256.0     1.0000   0.0000      106\n",
        "10.202064  10.154931       512    512.0     1.0000   0.0000      115\n",
        "10.121961  10.041859      1024   1024.0     1.0000   0.0000        2\n",
        "10.006912  9.891863       2048   2048.0     1.0000   0.0000      129\n",
        "9.912872   9.818832       4096   4096.0     1.0000   0.0000      120\n",
        "9.849909   9.786946       8192   8192.0     1.0000   0.0000       36\n",
        "9.826325   9.802741      16384  16384.0     1.0000   0.0000       90\n",
        "9.699268   9.572210      32768  32768.0     1.0000   0.0000       10\n",
        "9.613219   9.527170      65536  65536.0     1.0000   0.0000      696\n",
        "9.549665   9.486111     131072 131072.0     1.0000   0.0000       90\n",
        "9.637617   9.725570     262144 262144.0     1.0000   0.0000       84\n",
        "9.662667   9.687716     524288 524288.0     1.0000   0.0000       36\n",
        "9.652508   9.642349    1048576 1048576.0     1.0000   0.0000       39\n",
        "\n",
        "finished run\n",
        "number of examples = 1232530\n",
        "weighted example sum = 1.23253e+006\n",
        "weighted label sum = 1.23253e+006\n",
        "average loss = 0 h\n",
        "best constant = 1\n",
        "best constant's loss = 0\n",
        "total feature number = 151825390\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#my_tokenizer('text asd asddasf regtr gtr')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>doc_freq</th>\n",
        "      <th>token_score</th>\n",
        "      <th>doc_fraction</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 284478.000000</td>\n",
        "      <td> 284478.000000</td>\n",
        "      <td> 284478.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>      5.649934</td>\n",
        "      <td>     17.667187</td>\n",
        "      <td>      0.006341</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>     24.026474</td>\n",
        "      <td>    617.944969</td>\n",
        "      <td>      0.026966</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      0.001122</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      0.001122</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      0.001122</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>      2.000000</td>\n",
        "      <td>      3.000000</td>\n",
        "      <td>      0.002245</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>    803.000000</td>\n",
        "      <td> 284011.000000</td>\n",
        "      <td>      0.901235</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "            doc_freq    token_score   doc_fraction\n",
        "count  284478.000000  284478.000000  284478.000000\n",
        "mean        5.649934      17.667187       0.006341\n",
        "std        24.026474     617.944969       0.026966\n",
        "min         1.000000       1.000000       0.001122\n",
        "25%         1.000000       1.000000       0.001122\n",
        "50%         1.000000       1.000000       0.001122\n",
        "75%         2.000000       3.000000       0.002245\n",
        "max       803.000000  284011.000000       0.901235"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "__slotnames__\n",
        "__module__\n",
        "__doc__\n",
        "__init__\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rosetta.text.vw_helpers import LDAResults\n",
      "import rosetta.text.text_processors\n",
      "num_topics = 20\n",
      "#sff = SFileFilter(VWFormatter())\n",
      "#sff.load_sfile('sff_file.pkl')\n",
      "lda = LDAResults('topics_all.dat', 'prediction_all.dat', sff,\n",
      "                 num_topics=num_topics)\n",
      "lda.print_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "========== Printing top 5 tokens in every topic==========\n",
        "------------------------------\n",
        "Topic name: topic_00.  P[topic_00] = 0.0055\n",
        "                 topic_00  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_01.  P[topic_01] = 0.0478\n",
        "       topic_01  doc_freq\n",
        "token                    \n",
        "die    0.013462      6397\n",
        "van    0.011309      4140\n",
        "een    0.010391      1785\n",
        "het    0.010311      1753\n",
        "der    0.009824      3224\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_02.  P[topic_02] = 0.0055\n",
        "                 topic_02  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_03.  P[topic_03] = 0.0055\n",
        "                 topic_03  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_04.  P[topic_04] = 0.0055\n",
        "                 topic_04  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_05.  P[topic_05] = 0.0055\n",
        "                 topic_05  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_06.  P[topic_06] = 0.0055\n",
        "                 topic_06  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_07.  P[topic_07] = 0.0055\n",
        "                 topic_07  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_08.  P[topic_08] = 0.0055\n",
        "                 topic_08  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_09.  P[topic_09] = 0.0055\n",
        "                 topic_09  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_10.  P[topic_10] = 0.0912\n",
        "         topic_10  doc_freq\n",
        "token                      \n",
        "https    0.595310     99650\n",
        "via      0.019518     37705\n",
        "bir      0.004335      1544\n",
        "&amp     0.003770     41257\n",
        "stories  0.003669      4166\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_11.  P[topic_11] = 0.1233\n",
        "       topic_11  doc_freq\n",
        "token                    \n",
        "http   0.036115     64606\n",
        "que    0.029066     16914\n",
        "para   0.013619     12286\n",
        "por    0.009285     10375\n",
        "con    0.009176     10172\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_12.  P[topic_12] = 0.6490\n",
        "       topic_12  doc_freq\n",
        "token                    \n",
        "http   0.049371     64606\n",
        "new    0.007165     48825\n",
        "out    0.005722     45901\n",
        "&amp   0.005479     41257\n",
        "more   0.004741     37767\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_13.  P[topic_13] = 0.0055\n",
        "                 topic_13  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_14.  P[topic_14] = 0.0055\n",
        "                 topic_14  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_15.  P[topic_15] = 0.0055\n",
        "                 topic_15  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_16.  P[topic_16] = 0.0055\n",
        "                 topic_16  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_17.  P[topic_17] = 0.0055\n",
        "                 topic_17  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_18.  P[topic_18] = 0.0055\n",
        "                 topic_18  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n",
        "\n",
        "------------------------------\n",
        "Topic name: topic_19.  P[topic_19] = 0.0055\n",
        "                 topic_19  doc_freq\n",
        "token                              \n",
        "assalamualaikum  0.000008        30\n",
        "kpmg             0.000007        51\n",
        "siiiii           0.000007        24\n",
        "rocks.           0.000007        10\n",
        "haaaa            0.000006        26\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?sff.filter_extremes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sff.vocab_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using gensim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import nltk\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from string import digits\n",
      "import langid\n",
      "from gensim import corpora, models, similarities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "https://github.com/alexperrier/datatalks/blob/master/twitter/twitter_preprocessing.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_lang(lang, documents):\n",
      "    doclang = [  langid.classify(doc[1]) for doc in documents ]\n",
      "    return [documents[k] for k in range(len(documents)) if doclang[k][0] == lang]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "#path_list = ['all_texts/' + str(x) for x in pd.read_csv('first_level_sample_list_exists')['0']]\n",
      "path_list = ['all_texts/' + x for x in os.listdir('all_texts')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#path_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "documents = []\n",
      "for doc_path in path_list:\n",
      "    text = ''\n",
      "    with codecs.open(doc_path, encoding='utf-8') as f:\n",
      "        for line in f:\n",
      "            text += line + ' '\n",
      "    documents.append((doc_path, text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "138165"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_path = 'all'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  Filter non english documents\n",
      "documents = filter_lang('en', documents)\n",
      "print(\"We have \" + str(len(documents)) + \" documents in english \")\n",
      "\n",
      "# Remove urls\n",
      "documents = [(doc[0], re.sub(r\"(?:\\@|http?\\://)\\S+\", \"\", doc[1]))\n",
      "                for doc in documents ]\n",
      "\n",
      "# Remove documents with less 100 words (some timeline are only composed of URLs)\n",
      "documents = [doc for doc in documents if len(doc[1]) > 100]\n",
      "\n",
      "# tokenize\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "documents = [ (doc[0], tokenizer.tokenize(doc[1].lower())) for doc in documents ]\n",
      "\n",
      "# Remove stop words\n",
      "stoplist_tw=['amp','get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
      "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via',\n",
      "            'one','com','new','like','great','make','top','awesome','best',\n",
      "            'good','wow','yes','say','yay','would','thanks','thank','going',\n",
      "            'new','use','should','could','best','really','see','want','nice',\n",
      "            'while','know']\n",
      "\n",
      "#unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
      "#bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
      "\n",
      "stoplist  = set(nltk.corpus.stopwords.words(\"english\") + stoplist_tw)# + unigrams + bigrams)\n",
      "documents = [(doc[0], [token for token in doc[1] if (token not in stoplist and len(token) > 3)])\n",
      "                for doc in documents]\n",
      "\n",
      "# rm numbers only words\n",
      "documents = [ (doc[0], [token for token in doc[1] if len(token.strip(digits)) == len(token)])\n",
      "                for doc in documents ]\n",
      "\n",
      "# Lammetization\n",
      "# This did not add coherence ot the model and obfuscates interpretability of the\n",
      "# Topics. It was not used in the final model.\n",
      "#   from nltk.stem import WordNetLemmatizer\n",
      "#   lmtzr = WordNetLemmatizer()\n",
      "#   documents=[[lmtzr.lemmatize(token) for token in doc ] for doc in documents]\n",
      "\n",
      "# Remove words that only occur once\n",
      "#token_frequency = defaultdict(int)\n",
      "\n",
      "# count all token\n",
      "# for doc in documents:\n",
      "#     for token in doc:\n",
      "#         token_frequency[token] += 1\n",
      "\n",
      "# keep words that occur more than once\n",
      "# documents = [ [token for token in doc if token_frequency[token] > 1]\n",
      "#                 for doc in documents  ]\n",
      "\n",
      "# Sort words in documents\n",
      "for doc in documents:\n",
      "    doc[1].sort()\n",
      "\n",
      "words_only = [doc[1] for doc in documents]\n",
      "paths_only = [doc[0] for doc in documents]\n",
      "    \n",
      "# Build a dictionary where for each document each word has its own id\n",
      "dictionary = corpora.Dictionary(words_only)\n",
      "dictionary.filter_extremes(no_below=5, no_above=0.8)#, keep_n=100000)\n",
      "dictionary.compactify()\n",
      "# and save the dictionary for future use\n",
      "dictionary.save(target_path + '/corpora.dict')\n",
      "\n",
      "# We now have a dictionary with 26652 unique tokens\n",
      "#print(dictionary)\n",
      "\n",
      "# Build the corpus: vectors with occurence of each word for each document\n",
      "# convert tokenized documents to vectors\n",
      "corpus = [dictionary.doc2bow(doc) for doc in words_only]\n",
      "\n",
      "# and save in Market Matrix format\n",
      "corpora.MmCorpus.serialize(target_path + '/corpora_corpus.mm', corpus)\n",
      "# this corpus can be loaded with corpus = corpora.MmCorpus('alexip_followers.mm')\n",
      "with open(target_path + '/corpus_paths.dat', 'w') as f:\n",
      "    for path in paths_only:\n",
      "        f.write(path + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We have 87931 documents in english \n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#corpus = [dictionary.doc2bow(' '.join(doc)) for doc in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#documents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#words_only2 = [[str(x) for x in doc] for doc in words_only]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dictionary = corpora.Dictionary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "#Initialize Parameters\n",
      "corpus_filename = target_path + '/corpora_corpus.mm'\n",
      "dict_filename   = target_path + '/corpora.dict'\n",
      "lda_filename    = target_path + '/corpora.lda'\n",
      "lda_params      = {'num_topics': 40, 'passes': 20, 'alpha': 0.001}\n",
      "\n",
      "#print(\"Corpus of %s documents\" % len(documents))\n",
      "\n",
      "# Load the corpus and Dictionary\n",
      "#corpus = corpora.MmCorpus(corpus_filename)\n",
      "#dictionary = corpora.Dictionary.load(dict_filename)\n",
      "\n",
      "print(\"Running LDA with: %s  \" % lda_params)\n",
      "lda = models.LdaModel(corpus, id2word=dictionary,\n",
      "                        num_topics=lda_params['num_topics'],\n",
      "                        passes=lda_params['passes'],\n",
      "                        alpha = lda_params['alpha'])\n",
      "#print()\n",
      "lda.print_topics()\n",
      "lda.save(lda_filename)\n",
      "print(\"lda saved in %s \" % lda_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running LDA with: {'passes': 20, 'num_topics': 40, 'alpha': 0.001}  \n",
        "lda saved in all/corpora.lda "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction = np.zeros((len(corpus), lda_params['num_topics']))\n",
      "for doc_i in range(len(corpus)):\n",
      "    doc = corpus[doc_i]\n",
      "    for k,v in lda.get_document_topics(doc):\n",
      "        prediction[doc_i, k] = v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction_df = pd.DataFrame(prediction, columns=['topic_' + str(x) for x in range(lda_params['num_topics'])], index = [int(x.split('/')[1]) for x in paths_only])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction_df.to_csv(target_path + '/corpora_predictions.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(path_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "891"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "lda_vw = gensim.models.wrappers.LdaVowpalWabbit('vw.exe',\n",
      "                                                 corpus=corpus,\n",
      "                                                 num_topics=40,\n",
      "                                                 passes=20,\n",
      "                                                 alpha=0.001,\n",
      "                                                 id2word=dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_vw.save('alexip_vw.lda')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_vw.print_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[u'0.030*neo4j + 0.029*nigeria + 0.015*free + 0.011*nigerian + 0.011*africa + 0.010*http + 0.009*america + 0.008*british + 0.008*protest + 0.007*police',\n",
        " u'0.040*djangocon + 0.013*duth + 0.012*auch + 0.011*aber + 0.009*nicht + 0.007*wien + 0.007*dann + 0.006*dass + 0.006*mehr + 0.004*vienna',\n",
        " u'0.021*niet + 0.019*voor + 0.012*maar + 0.009*mijn + 0.008*zijn + 0.008*mysql + 0.006*mongodb + 0.006*naar + 0.006*geen + 0.005*clojure',\n",
        " u'0.019*career + 0.019*jobs + 0.018*tech + 0.016*ireland + 0.014*dublin + 0.014*featuring + 0.012*check + 0.012*looking + 0.010*channel + 0.009*irish',\n",
        " u'0.001*bless + 0.000*christmas + 0.000*think + 0.000*nothing + 0.000*early + 0.000*research + 0.000*mind + 0.000*getting + 0.000*windows + 0.000*times',\n",
        " u'0.070*firefox + 0.052*help + 0.025*sorry + 0.022*tweet + 0.020*might + 0.019*inconvenience + 0.018*query + 0.017*problem + 0.014*using + 0.013*need',\n",
        " u'0.064*neo4j + 0.053*graph + 0.020*meetup + 0.013*cypher + 0.010*michael + 0.009*graphs + 0.009*query + 0.008*graphdb + 0.008*docker + 0.005*nodes',\n",
        " u'0.034*bitcoin + 0.019*security + 0.016*nasa + 0.013*blockchain + 0.006*market + 0.006*malware + 0.005*agora + 0.005*privacy + 0.004*time + 0.004*attack',\n",
        " u'0.078*datascience + 0.050*data + 0.041*science + 0.037*learning + 0.034*python + 0.028*machinelearning + 0.022*bigdata + 0.022*deep + 0.016*machine + 0.014*spark',\n",
        " u'0.113*video + 0.058*playlist + 0.053*added + 0.052*liked + 0.033*india + 0.031*official + 0.017*full + 0.014*song + 0.012*songs + 0.009*music']"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gensim.models.wrappers.ldavowpalwabbit.write_corpus_as_vw(corpus, 'corpus.vw')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "542"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw.exe --lda 40 --cache_file ddrs.cache --passes 20 -p prediction_all.dat --readable_model topics_all.dat --bit_precision 18 corpus.vw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Num weight bits = 18\n",
        "learning rate = 0.5\n",
        "initial_t = 0\n",
        "power_t = 0.5\n",
        "decay_learning_rate = 1\n",
        "predictions = prediction_all.dat\n",
        "can't open: ddrs.cache, error = No such file or directory\n",
        "creating cache_file = ddrs.cache\n",
        "Reading datafile = corpus.vw\n",
        "num sources = 1\n",
        "average    since         example     example  current  current  current\n",
        "loss       last          counter      weight    label  predict features\n",
        "13.396970  13.396970         1      1.0    unknown   0.0000       55\n",
        "13.412948  13.428926         2      2.0    unknown   0.0000       22\n",
        "12.983844  12.554740         4      4.0    unknown   0.0000     2869\n",
        "11.229764  9.475684          8      8.0    unknown   0.0000      861\n",
        "10.958396  10.687027        16     16.0    unknown   0.0000        8\n",
        "10.617364  10.276332        32     32.0    unknown   0.0000       84\n",
        "10.283157  9.948950         64     64.0    unknown   0.0000      502\n",
        "10.011421  9.739684        128    128.0    unknown   0.0000      510\n",
        "9.828065   9.644709        256    256.0    unknown   0.0000     3398\n",
        "9.711307   9.594549        512    512.0    unknown   0.0000       40\n",
        "9.631405   9.551503       1024   1024.0    unknown   0.0000     4884\n",
        "9.562946   9.494486       2048   2048.0    unknown   0.0000       60\n",
        "9.508503   9.454060       4096   4096.0    unknown   0.0000      139\n",
        "9.462755   9.417007       8192   8192.0    unknown   0.0000     3838\n",
        "\n",
        "finished run\n",
        "number of examples = 9760\n",
        "weighted example sum = 9760\n",
        "weighted label sum = 0\n",
        "average loss = 0 h\n",
        "total feature number = 13942020\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}